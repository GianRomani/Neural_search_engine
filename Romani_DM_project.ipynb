{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GianRomani/Neural_search_engine/blob/main/Romani_DM_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not working on Colab, it should work in local. It is needed to use GPU with multiprocess (in Flow)\n",
        "%env JINA_MP_START_METHOD='spawn'\n",
        "import os\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "print(os.getenv(\"JINA_MP_START_METHOD\"))\n",
        "mp.set_start_method('spawn', force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psi4XQJzqmoC",
        "outputId": "2c8484f4-bdb7-4dc0-f919-223a3e91fd53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: JINA_MP_START_METHOD='spawn'\n",
            "'spawn'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btPirC-5-5B7",
        "outputId": "61b41601-5b64-4f9c-d95a-96827140af0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try: \n",
        "  #this will raise an error if you are not using Colab\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  print(\"You are not using Colab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p-1HyapB_rPx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = pd.read_csv(\"/content/drive/MyDrive/DM/Questions.csv\", encoding=\"ISO-8859-1\")\n",
        "answers = pd.read_csv(\"/content/drive/MyDrive/DM/Answers.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "index_outfile = \"/content/drive/MyDrive/DM/index_1k.csv\""
      ],
      "metadata": {
        "id": "okdIHgf7Qy-_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RbtMSoV-U_0"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZCHmmjqvb0b"
      },
      "source": [
        "It can be interesting and helpful to analyze:\n",
        "\n",
        "*   the distribution of the scores among the questions,\n",
        "*   number of answers for question (and scores of those answers);\n",
        "*   creation dates for the posts;\n",
        "*   if a post was closed (and why);\n",
        "*   popular users (who answered a lot of questions)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aMmDu1Dpj8q"
      },
      "source": [
        "<h2>Questions</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NupnZcXjyjfN"
      },
      "source": [
        "*   Id: id of the question;\n",
        "*   OwnerUserId: id of the account who post the question;\n",
        "*   CreationDate: when the question was posted;\n",
        "*   ClosedDate: when the question was closed;\n",
        "*   Score: upvotes - downvotes;\n",
        "*   Title: title of the question;\n",
        "*   Body: the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEwlkjWtbWKK"
      },
      "outputs": [],
      "source": [
        "questions.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wspAqCIigkdV"
      },
      "outputs": [],
      "source": [
        "questions.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgynb7ZUflJN"
      },
      "outputs": [],
      "source": [
        "#What is the distribution of the scores among the questions?\n",
        "score_distribution = questions[\"Score\"].value_counts(bins=20)\n",
        "score_distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBVv6gSHq1PG"
      },
      "outputs": [],
      "source": [
        "#negative scores\n",
        "negative_scores_questions = questions[questions[\"Score\"] < 0]\n",
        "negative_scores_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXPO7_PqiAqV"
      },
      "outputs": [],
      "source": [
        "#Posts with a score greater than 4000\n",
        "best_posts = questions[questions[\"Score\"] >4000]\n",
        "best_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nRsYk40qI-q"
      },
      "outputs": [],
      "source": [
        "#Count NaN values -> most of them in ClosedDate\n",
        "questions.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dugxDhRlpezf"
      },
      "source": [
        "<h2>Answers</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjCVejxuyFPl"
      },
      "source": [
        "\n",
        "*   Id: id of the answer;\n",
        "*   OwnerUserId: id of the account who post the answer;\n",
        "*   CreationDate: when the answer was posted;\n",
        "*   ParentId: Id of the question;\n",
        "*   Score: upvotes - downvotes;\n",
        "*   Body: the answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHzWWfaNFggg"
      },
      "outputs": [],
      "source": [
        "answers.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfTUfpAEFSSV"
      },
      "outputs": [],
      "source": [
        "answers.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ni0sADlFrMb"
      },
      "outputs": [],
      "source": [
        "#Distribution for the scores of the answers\n",
        "score_distribution = answers[\"Score\"].value_counts(bins=20)\n",
        "score_distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ4-a09Iw0DO"
      },
      "outputs": [],
      "source": [
        "#negative scores\n",
        "negative_scores_answers = answers[answers[\"Score\"] < 0]\n",
        "negative_scores_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBb1Bk8yGM3w"
      },
      "outputs": [],
      "source": [
        "#Number of answers for question\n",
        "answers_for_post = answers.groupby([\"ParentId\"]).count()[\"Id\"]\n",
        "answers_for_post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF1p9H25uNQN"
      },
      "outputs": [],
      "source": [
        "#max number of answers for a post\n",
        "answers_for_post.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH9S_AVPuFeQ"
      },
      "outputs": [],
      "source": [
        "#How many questions have a certain number of answers?\n",
        "print(\"#answers   #questions\")\n",
        "answers_for_post.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEiYYRsyrql2"
      },
      "outputs": [],
      "source": [
        "#How many NaN values?\n",
        "answers.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhH_hoaHx4nS"
      },
      "outputs": [],
      "source": [
        "#How many questions were answered by the users?\n",
        "users_and_number_of_answers = answers[\"OwnerUserId\"].value_counts()\n",
        "print(\"UserId     #answers\")\n",
        "users_and_number_of_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UKM1Ol81yHo"
      },
      "outputs": [],
      "source": [
        "#Distribution of the number of answers posted by different users\n",
        "print(\"#answers    #users\")\n",
        "users_and_number_of_answers.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKi7XF15pqBw"
      },
      "source": [
        "<h2>Tags</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gcYNdLe40yH"
      },
      "source": [
        "\n",
        "*   Id: id of the question\n",
        "*   Tag: just the tag :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdfkaejx4oGi"
      },
      "outputs": [],
      "source": [
        "tags.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suT2aso54w2v"
      },
      "outputs": [],
      "source": [
        "#How many rows?\n",
        "tags.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8bgqlxp7FtF"
      },
      "outputs": [],
      "source": [
        "#Check if there are some null values\n",
        "tags.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRmlKKwW5L8W"
      },
      "outputs": [],
      "source": [
        "#Number of tags for post\n",
        "number_of_tags_for_post = tags[\"Id\"].value_counts()\n",
        "print(\"Post     #tags\")\n",
        "number_of_tags_for_post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGqbH65N70B9"
      },
      "outputs": [],
      "source": [
        "#Distribution of number of tags\n",
        "number_of_tags_for_post.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InKl0Ru19ilk"
      },
      "outputs": [],
      "source": [
        "#Number of tags for post\n",
        "tags_used = tags[\"Tag\"].value_counts()\n",
        "tags_used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJAHeGyPrI3i"
      },
      "outputs": [],
      "source": [
        "counter = 0\n",
        "for i in tags_used:\n",
        "  if i > 10:\n",
        "    counter+=1\n",
        "counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0pGmlDf-QvZ"
      },
      "outputs": [],
      "source": [
        "#How many tags appear a certain number of times\n",
        "print(\"#occurrences #tags\")\n",
        "occ_tags = tags_used.value_counts()\n",
        "occ_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTMq1mah8-nC"
      },
      "outputs": [],
      "source": [
        "#distribution of how many tags appear a certain number of times\n",
        "tags_used_groups = tags_used.value_counts(bins=20)\n",
        "tags_used_groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxw8aRjTWf9w"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2UUCjgl4ndC",
        "outputId": "acc655cb-52ec-408e-b798-3d48ac72562f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aQn-C3y14OcC"
      },
      "outputs": [],
      "source": [
        "def cleanpunc(sentence): \n",
        "    no_punct = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    return  no_punct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JcP2XtWi4l1a"
      },
      "outputs": [],
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"\\n\", \" \", phrase)\n",
        "    return phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YMCznUT5PTR",
        "outputId": "0657e736-90b6-415a-acc9-5a2fc48d50de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list of stop words: {\"wouldn't\", 'o', \"mustn't\", 'ourselves', 'by', 'further', 'just', 'above', 'mightn', 'yourselves', 'over', 'through', 'we', 'don', 'should', 'during', 'a', \"should've\", 'it', 'more', \"didn't\", \"doesn't\", \"it's\", 'too', 'wouldn', 'as', 'any', 'he', 'themselves', 'the', 'his', 'didn', 'hers', 'again', \"haven't\", \"isn't\", 'have', 'under', 'are', 'myself', 'be', \"needn't\", \"hadn't\", 'will', 'being', 'me', 'having', 'am', \"you're\", 'she', 'him', 'against', 'why', \"you've\", 'such', 'y', 'about', 'own', 'your', 'm', 'nor', 'other', 'doesn', 'below', 'because', 'then', 'can', 'after', 'at', \"mightn't\", 'off', 'this', 'now', \"shan't\", 'both', 'hadn', 'were', \"wasn't\", 'but', 'than', 'ours', 'those', 'to', 'their', 'my', 'into', \"couldn't\", 'there', 'yourself', 'needn', 'yours', 'll', 'had', 'between', 'was', 'from', 'what', 'when', 'itself', 're', 'has', 'shan', 'out', 'himself', 'once', 'very', 's', 'how', 'theirs', 'for', 'been', 'doing', 'aren', 'mustn', 'haven', 'so', 'of', 'few', 'while', \"you'd\", 'whom', 'is', 'isn', 've', 'or', 'only', \"aren't\", 'did', 'does', 'an', 'our', 'her', 'd', 'who', 'with', 'each', \"shouldn't\", 'these', 'them', 'on', \"won't\", 'they', \"hasn't\", 'i', 'won', 'if', 'same', 'hasn', 'most', 'not', 'ain', \"don't\", 'no', 'shouldn', 'couldn', 'which', 'herself', 'do', 'you', \"she's\", \"that'll\", 't', 'in', 'here', 'wasn', \"you'll\", 'weren', 'all', 'some', 'until', 'up', 'before', 'ma', \"weren't\", 'where', 'its', 'down', 'that', 'and'}\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print ('list of stop words:', stop_words)\n",
        "\n",
        "def clean_stop_words(total_text):\n",
        "    if type(total_text) is not int:\n",
        "        string = \"\"\n",
        "        for word in total_text.split():\n",
        "            if not word in stop_words:\n",
        "                string += word + \" \"\n",
        "        return string  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mgb-nStZFVoy"
      },
      "outputs": [],
      "source": [
        "def preprocessing_operations(text):\n",
        "  splitted_text_code = re.split('<pre><code>|</code></pre>', text)\n",
        "  res = []\n",
        "  for idx, text in enumerate(splitted_text_code):\n",
        "    if idx%2 == 0:\n",
        "      no_html = soup(text, \"lxml\").text\n",
        "      no_punctuation = cleanpunc(no_html)\n",
        "      no_contractions = decontracted(no_punctuation)\n",
        "      no_stopwords = clean_stop_words(no_contractions)\n",
        "      res.append(no_stopwords)\n",
        "    else:\n",
        "      res.append(text)\n",
        "  return res\n",
        "\n",
        "def preprocess_data(data):\n",
        "  cleaned_text = []\n",
        "  if type(data)==type(\"string\"): #preprocess a single text\n",
        "    cleaned_text = preprocessing_operations(data)\n",
        "  else: #preprocess sevaral texts in input\n",
        "    body = data[\"Body\"]\n",
        "    for q in body:\n",
        "      cleaned_text.append(preprocessing_operations(q))\n",
        "  return cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wus1mVOuh710"
      },
      "source": [
        "# Jina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T95C16q5jmWT",
        "outputId": "1f0d27b3-7952-4e67-9534-f124a857b128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jina\n",
            "  Downloading jina-2.6.4.tar.gz (864 kB)\n",
            "\u001b[K     |████████████████████████████████| 864 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.33.1 in /usr/local/lib/python3.7/dist-packages (from jina) (1.43.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jina) (2.23.0)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 43.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.7/dist-packages (from jina) (22.3.0)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jina) (1.19.5)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 39.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.3 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.73.0-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 569 kB/s \n",
            "\u001b[?25hCollecting rich\n",
            "  Downloading rich-11.2.0-py3-none-any.whl (217 kB)\n",
            "\u001b[K     |████████████████████████████████| 217 kB 53.3 MB/s \n",
            "\u001b[?25hCollecting docker\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting pathspec\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting uvicorn[standard]>=0.14.0\n",
            "  Downloading uvicorn-0.17.4-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting aiostream\n",
            "  Downloading aiostream-0.4.4.tar.gz (32 kB)\n",
            "Requirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from jina) (3.17.3)\n",
            "Collecting uvloop\n",
            "  Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 44.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from jina) (3.4.2)\n",
            "Collecting websockets\n",
            "  Downloading websockets-10.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 48.6 MB/s \n",
            "\u001b[?25hCollecting kubernetes>=18.20.0\n",
            "  Downloading kubernetes-21.7.0-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 40.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from jina) (5.1.1)\n",
            "Collecting lz4<3.1.2\n",
            "  Downloading lz4-3.1.1-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 44.5 MB/s \n",
            "\u001b[?25hCollecting aiofiles\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.33.1->jina) (1.15.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from kubernetes>=18.20.0->jina) (2021.10.8)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from kubernetes>=18.20.0->jina) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from kubernetes>=18.20.0->jina) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.7/dist-packages (from kubernetes>=18.20.0->jina) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes>=18.20.0->jina) (1.3.1)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from kubernetes>=18.20.0->jina) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.0.1->kubernetes>=18.20.0->jina) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.0.1->kubernetes>=18.20.0->jina) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.0.1->kubernetes>=18.20.0->jina) (0.2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=18.20.0->jina) (0.4.8)\n",
            "Collecting asgiref>=3.4.0\n",
            "  Downloading asgiref-3.5.0-py3-none-any.whl (22 kB)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn[standard]>=0.14.0->jina) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from uvicorn[standard]>=0.14.0->jina) (3.10.0.2)\n",
            "Collecting httptools<0.4.0,>=0.2.0\n",
            "  Downloading httptools-0.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 51.0 MB/s \n",
            "\u001b[?25hCollecting python-dotenv>=0.13\n",
            "  Downloading python_dotenv-0.19.2-py2.py3-none-any.whl (17 kB)\n",
            "Collecting watchgod>=0.6\n",
            "  Downloading watchgod-0.7-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 51.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 54.2 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina) (2.0.11)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->jina) (2.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->jina) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->jina) (2.21)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jina) (3.0.4)\n",
            "Collecting starlette==0.17.1\n",
            "  Downloading starlette-0.17.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting anyio<4,>=3.0.0\n",
            "  Downloading anyio-3.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes>=18.20.0->jina) (3.2.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->jina) (2.6.1)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: jina, aiostream, python-multipart\n",
            "  Building wheel for jina (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jina: filename=jina-2.6.4-py3-none-any.whl size=966749 sha256=dbb8224340952f8e3bfa2aae12f1e7ad3ccddc650fe1658e28fff1cc92a9deaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/12/25/20bdf398b555a3c698b4974c6bc613878b8fab0f7863775b83\n",
            "  Building wheel for aiostream (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aiostream: filename=aiostream-0.4.4-py3-none-any.whl size=35648 sha256=59e5c752a5bb2d843180a05173215396d7c40c28519eb990604b5461ec915d81\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/21/19/0141f098b792ee743e77c609bc0d42e7742b1f66db58f56cd9\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=50917cfb02bed36d828badb5e41b22367ece3d0f1113f35e219b13644d80feca\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built jina aiostream python-multipart\n",
            "Installing collected packages: sniffio, multidict, h11, frozenlist, asgiref, anyio, yarl, websockets, websocket-client, watchgod, uvloop, uvicorn, starlette, pyyaml, python-dotenv, pydantic, httptools, commonmark, colorama, asynctest, async-timeout, aiosignal, rich, python-multipart, pathspec, lz4, kubernetes, fastapi, docker, cryptography, aiostream, aiohttp, aiofiles, jina\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiofiles-0.8.0 aiohttp-3.8.1 aiosignal-1.2.0 aiostream-0.4.4 anyio-3.5.0 asgiref-3.5.0 async-timeout-4.0.2 asynctest-0.13.0 colorama-0.4.4 commonmark-0.9.1 cryptography-36.0.1 docker-5.0.3 fastapi-0.73.0 frozenlist-1.3.0 h11-0.13.0 httptools-0.3.0 jina-2.6.4 kubernetes-21.7.0 lz4-3.1.1 multidict-6.0.2 pathspec-0.9.0 pydantic-1.9.0 python-dotenv-0.19.2 python-multipart-0.0.5 pyyaml-6.0 rich-11.2.0 sniffio-1.2.0 starlette-0.17.1 uvicorn-0.17.4 uvloop-0.16.0 watchgod-0.7 websocket-client-1.2.3 websockets-10.1 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install jina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "52I4frUGu5_Q",
        "outputId": "6f3cbf94-847d-4001-a9fb-f7a523dced3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 34.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 28.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 39.2 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 10.6 MB/s \n",
            "\u001b[?25hCollecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 20.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.43.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.11)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=228cb51bf360b0116d54f07ffaaa669950f7d9f548eb6525cb366d3b092b32bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: setuptools, pyDeprecate, fsspec, torchmetrics, tokenizers, sacremoses, huggingface-hub, future, transformers, pytorch-lightning\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed fsspec-2022.1.0 future-0.18.2 huggingface-hub-0.4.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.10 sacremoses-0.0.47 setuptools-59.5.0 tokenizers-0.11.4 torchmetrics-0.7.2 transformers-4.16.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install \"transformers\" \"pytorch-lightning\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCtiGYEq7V1B",
        "outputId": "18ade15b-c463-4077-98e1-b0009175a269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre dkms freeglut3 freeglut3-dev\n",
            "  keyboard-configuration libargon2-0 libcap2 libcryptsetup12\n",
            "  libdevmapper1.02.1 libfontenc1 libidn11 libip4tc0 libjansson4\n",
            "  libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxfont2\n",
            "  libxi-dev libxkbfile1 libxmu-dev libxmu-headers libxnvctrl0 libxtst6\n",
            "  nsight-compute-2020.2.1 nsight-compute-2022.1.0 nsight-systems-2020.3.2\n",
            "  nsight-systems-2020.3.4 nsight-systems-2021.5.2 nvidia-dkms-510\n",
            "  nvidia-kernel-common-510 nvidia-kernel-source-510 nvidia-modprobe\n",
            "  nvidia-settings openjdk-11-jre policykit-1 policykit-1-gnome python3-xkit\n",
            "  screen-resolution-extra systemd systemd-sysv udev x11-xkb-utils\n",
            "  xserver-common xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  libomp5\n",
            "Suggested packages:\n",
            "  libomp-doc\n",
            "The following NEW packages will be installed:\n",
            "  libomp-dev libomp5\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 239 kB of archives.\n",
            "After this operation, 804 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp-dev amd64 5.0.1-1 [5,088 B]\n",
            "Fetched 239 kB in 1s (290 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Selecting previously unselected package libomp-dev.\n",
            "Preparing to unpack .../libomp-dev_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp-dev (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp-dev (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting faiss\n",
            "  Downloading faiss-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.19.5)\n",
            "Installing collected packages: faiss\n",
            "Successfully installed faiss-1.5.3\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 85.5 MB 148 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!apt install libomp-dev\n",
        "!pip install faiss\n",
        "!pip install --upgrade faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0bif-D0NqnPx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from __future__ import division\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from jina import Flow, Executor, requests, Document, DocumentArray, DocumentArrayMemmap\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frEE5DlmB6d1",
        "outputId": "aa73d144-6365-4f67-c882-8524b10dd3d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ],
      "source": [
        "device = 0\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piUtOfu_f5rH"
      },
      "source": [
        "The authors of CodeBert set the MAX_LENGTH for the sentences to 512(+2) so, if the input vector is too big, I can not compute the embeddings -> two possible solutions (https://github.com/microsoft/CodeBERT/issues/16):\n",
        "\n",
        "*   use an RNN over k representations to obtain a final embedding;\n",
        "*   take the average of the representations.\n",
        "\n",
        "To use the cosine distance we need vectors that have same length ->there are two ways to obtain a representation of same length for the texts:\n",
        "\n",
        "*   use the [CLS] token;\n",
        "*   use the average of the vectors\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b2TY3cvPj9FC"
      },
      "outputs": [],
      "source": [
        "class MyTransformer(Executor):\n",
        "    \"\"\"Transformer executor class \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            pretrained_model_name_or_path: str = \"microsoft/codebert-base\",\n",
        "            *args,\n",
        "            device: str = 'cpu',\n",
        "            **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "        self.model = AutoModel.from_pretrained(pretrained_model_name_or_path)\n",
        "        self.model.to(device)\n",
        "        print(\"Device used: {}\".format(self.model.device))\n",
        "\n",
        "    def compute_embedding(self,tokens_ids):\n",
        "      if(len(tokens_ids) < 512):  \n",
        "        context_embeddings = self.model(torch.tensor(tokens_ids)[None,:].to(device))[0]\n",
        "      else:\n",
        "        k = ceil(len(tokens_ids) / 512.0)\n",
        "        chunks = np.array_split(tokens_ids, k)\n",
        "        chunks_tensor = torch.tensor(chunks[0])[None,:].to(device)\n",
        "        context_embeddings_list = self.model(chunks_tensor)[0].to(device)\n",
        "        for i in range(1, len(chunks)):\n",
        "          chunks_tensor = torch.tensor(chunks[i])[None,:].to(device)\n",
        "          context_embeddings_list = torch.cat((context_embeddings_list, self.model(chunks_tensor)[0]), 1)\n",
        "        dim = context_embeddings_list.shape[-1]\n",
        "        context_embeddings = context_embeddings_list.to(device)\n",
        "      #compute the average of the vectors\n",
        "      avg_embeddings = torch.mean(context_embeddings,1)[0]\n",
        "      return avg_embeddings\n",
        "\n",
        "    @requests\n",
        "    def encode(self, docs: 'DocumentArray', *args, **kwargs):\n",
        "        start = time.time()\n",
        "        tokens = [self.tokenizer.cls_token]\n",
        "        texts = docs.get_attributes(\"text\")\n",
        "        for idx, q in enumerate(texts): \n",
        "          cleaned_q = preprocess_data(q)\n",
        "          tokens = [self.tokenizer.cls_token]\n",
        "          for l in cleaned_q:\n",
        "            code_tokens = self.tokenizer.tokenize(l)\n",
        "            tokens += code_tokens\n",
        "            tokens += [self.tokenizer.sep_token]\n",
        "          tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "          docs[idx].embedding = self.compute_embedding(tokens_ids)\n",
        "        print(\"Time needed to encode {} documents: {}\".format(len(docs), time.time()-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SQMAhi2rnYnF"
      },
      "outputs": [],
      "source": [
        "class MyIndexer(Executor): #Indexer with Faiss library\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.index_output = None\n",
        "        self.index_file = None\n",
        "        self.index_output = kwargs[\"index_output\"]\n",
        "\n",
        "    @requests(on='/index')\n",
        "    def index(self, docs: DocumentArray, mode: str = None, **kwargs): \n",
        "      dim = docs.embeddings.shape[1]\n",
        "      nlist = 50  # how many cells\n",
        "      m = 8  # number of centroid IDs in final compressed vectors\n",
        "      bits = 8 # number of bits in each centroid\n",
        "\n",
        "      embeddings = docs.embeddings.cpu().detach().numpy()\n",
        "      quantizer = faiss.IndexFlatL2(dim)\n",
        "      self.my_index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, bits) \n",
        "\n",
        "      start = time.time()\n",
        "      self.my_index.train(embeddings)\n",
        "      self.my_index.add(embeddings)\n",
        "      print(\"Time needed to train and add {} texts to the index: {} s\".format(self.my_index.ntotal, time.time()-start))\n",
        "\n",
        "      if(mode == \"w\"):\n",
        "        #write index on disk\n",
        "        faiss.write_index(self.my_index, self.index_output)\n",
        "\n",
        "    @requests(on='/search')\n",
        "    def search(self, docs: DocumentArray, k:int = 4, **kwargs):\n",
        "        s, d = self.my_index.search(docs.embeddings.numpy(), k)\n",
        "        start = time.time()\n",
        "        print(\"Time needed to search {} query: {} s\".format(len(docs), time.time()-start))\n",
        "        print(\"Indexes of the most similar documents: {}\\nwith distance: {}\".format(d,s))\n",
        "        return d[0]\n",
        "          \n",
        "    def print_index(self):\n",
        "      for d in self.index:\n",
        "        print(\"Document id: {}\\nembedding: {} of shape: {}\".format(d.id, d.embedding, d.embedding.shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getKey(item):\n",
        "  return item[2]\n",
        "\n",
        "def get_answers(docIds: list, questions, answers):\n",
        "  for i, idx in enumerate(docIds):\n",
        "    if idx == -1: #This result is not valid\n",
        "      continue\n",
        "    print(\"{}th result:\".format(i+1))\n",
        "    #document\n",
        "    doc = questions.loc[idx]\n",
        "    doc_id = doc.Id\n",
        "    #question's title\n",
        "    title = doc[\"Title\"]\n",
        "    #question's body\n",
        "    body_q = doc[\"Body\"]\n",
        "    #answers\n",
        "    answers_q = answers.loc[answers.ParentId == doc_id]\n",
        "    answers.drop(answers[answers.Score <= 0].index, inplace=True)\n",
        "    if (answers_q.empty):\n",
        "      #skip this question on SO -> maybe add as feature the fact that other documents will be retrieved instead\n",
        "      continue\n",
        "    answers_texts = []\n",
        "    for a in answers_q.itertuples():\n",
        "      answers_texts.append((a.Id, a.Body, a.Score))\n",
        "    answers_texts_sorted = sorted(answers_texts, key=getKey, reverse=True)\n",
        "    print(\"Doc Id: {}\\nTitle: {}\\nText:\\n{}\".format(doc_id, title, body_q))\n",
        "    for j, a in enumerate(answers_texts_sorted):\n",
        "      print(\"----------------------------\")\n",
        "      print(\"{}th answer's Id: {}, with score: {} and text:\\n{}\".format(j+1, a[0], a[2], a[1]))\n",
        "    print(\"########################################\")"
      ],
      "metadata": {
        "id": "LHJ-SxjCOGxZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_with_gpu(num_docs: int = 100, query:str = None, **kwargs):\n",
        "  start = time.time()\n",
        "  #index\n",
        "  i = MyIndexer(index_output = kwargs[\"index_output\"], index_file = kwargs[\"index_file\"]) #pass operation = 'w' to write and save a new index on disk\n",
        "  if(num_docs == 0): #load the whole dataset\n",
        "    docArray = DocumentArray([Document(id=questions.iloc[i].Id, text=questions.iloc[i].Body) for i in range(len(questions))])\n",
        "    print(\"Time needed to read the whole dataset: {} s\".format(time.time()-start))\n",
        "  else:\n",
        "    docArray = DocumentArray([Document(id=questions.iloc[i].Id, text=questions.iloc[i].Body) for i in range(num_docs)])\n",
        "    print(\"Time needed to read {} texts: {} s\".format(num_docs, time.time()-start))\n",
        "    #embeddings\n",
        "    e = MyTransformer(device=device)\n",
        "    e.encode(docArray)\n",
        "    #create new index\n",
        "    i.index(docArray, mode=\"w\") \n",
        "  #query\n",
        "  if(query is None):\n",
        "    q = DocumentArray([Document(text=questions.iloc[1].Body)])\n",
        "    e.encode(q)\n",
        "  else:\n",
        "    q = DocumentArray([Document(text = query)])\n",
        "    e.encode(q)\n",
        "  #search\n",
        "  docs = i.search(q, k=5)\n",
        "  get_answers(docs, questions, answers)"
      ],
      "metadata": {
        "id": "ebleHJcnBTN1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PRws9l2ZkObO"
      },
      "outputs": [],
      "source": [
        "flow = (\n",
        "        Flow()\n",
        "            .add(name='MyTransformer', uses=MyTransformer, uses_with={'device': \"cuda:0\"}, gpus='all')\n",
        "            .add(name='MyIndexer', uses=MyIndexer)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x4l1alya2TxK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "b71a2746-bb84-4c8e-9344-55bb3afa86e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Flow@82[I]:flow visualization: https://mermaid.ink/svg/ICAgICAgICAgICAgJSV7aW5pdDp7ICAidGhlbWUiOiAiYmFzZSIsICAidGhlbWVWYXJpYWJsZXMiOiB7ICAgICAgInByaW1hcnlDb2xvciI6ICIjZmZmIiwgICAgICAicHJpbWFyeUJvcmRlckNvbG9yIjogIiNmZmYiLCAgICAgICJtYWluQmtnIjogIiMzMkM4Q0QiLCAgICAgICJjbHVzdGVyQmtnIjogIiNFRUVERTc4QyIsICAgICAgInNlY29uZGFyeUJvcmRlckNvbG9yIjogIm5vbmUiLCAgICAgICJ0ZXJ0aWFyeUJvcmRlckNvbG9yIjogIm5vbmUiLCAgICAgICJsaW5lQ29sb3IiOiAiI2E2ZDhkYSIgICAgICB9fX0lJSAgICAgICAgICAgIApmbG93Y2hhcnQgTFI7CnN1YmdyYXBoIE15VHJhbnNmb3JtZXI7Ck15VHJhbnNmb3JtZXIvcGVhLTBbTXlUcmFuc2Zvcm1lcl06OjpQRUE7CmVuZDsKc3ViZ3JhcGggTXlJbmRleGVyOwpNeUluZGV4ZXIvcGVhLTBbTXlJbmRleGVyXTo6OlBFQTsKZW5kOwpnYXRld2F5c3RhcnRbZ2F0ZXdheV06OjpHQVRFV0FZIC0tPiBNeVRyYW5zZm9ybWVyOjo6UE9EOwpNeVRyYW5zZm9ybWVyOjo6UE9EIC0tPiBNeUluZGV4ZXI6OjpQT0Q7Ck15SW5kZXhlcjo6OlBPRCAtLT4gZ2F0ZXdheWVuZFtnYXRld2F5XTo6OkdBVEVXQVk7CmNsYXNzRGVmIElOU1BFQ1Qgc3Ryb2tlOiNGMjlDOUYKY2xhc3NEZWYgSk9JTl9JTlNQRUNUIHN0cm9rZTojRjI5QzlGCmNsYXNzRGVmIEdBVEVXQVkgZmlsbDpub25lLGNvbG9yOiMwMDAsc3Ryb2tlOm5vbmUKY2xhc3NEZWYgSU5TUEVDVF9BVVhfUEFTUyBzdHJva2UtZGFzaGFycmF5OiAyIDIKY2xhc3NEZWYgSEVBRFRBSUwgZmlsbDojMzJDOENEMUQKCmNsYXNzRGVmIEVYVEVSTkFMIGZpbGw6I2ZmZixzdHJva2U6IzMyQzhDRA==\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/svg/ICAgICAgICAgICAgJSV7aW5pdDp7ICAidGhlbWUiOiAiYmFzZSIsICAidGhlbWVWYXJpYWJsZXMiOiB7ICAgICAgInByaW1hcnlDb2xvciI6ICIjZmZmIiwgICAgICAicHJpbWFyeUJvcmRlckNvbG9yIjogIiNmZmYiLCAgICAgICJtYWluQmtnIjogIiMzMkM4Q0QiLCAgICAgICJjbHVzdGVyQmtnIjogIiNFRUVERTc4QyIsICAgICAgInNlY29uZGFyeUJvcmRlckNvbG9yIjogIm5vbmUiLCAgICAgICJ0ZXJ0aWFyeUJvcmRlckNvbG9yIjogIm5vbmUiLCAgICAgICJsaW5lQ29sb3IiOiAiI2E2ZDhkYSIgICAgICB9fX0lJSAgICAgICAgICAgIApmbG93Y2hhcnQgTFI7CnN1YmdyYXBoIE15VHJhbnNmb3JtZXI7Ck15VHJhbnNmb3JtZXIvcGVhLTBbTXlUcmFuc2Zvcm1lcl06OjpQRUE7CmVuZDsKc3ViZ3JhcGggTXlJbmRleGVyOwpNeUluZGV4ZXIvcGVhLTBbTXlJbmRleGVyXTo6OlBFQTsKZW5kOwpnYXRld2F5c3RhcnRbZ2F0ZXdheV06OjpHQVRFV0FZIC0tPiBNeVRyYW5zZm9ybWVyOjo6UE9EOwpNeVRyYW5zZm9ybWVyOjo6UE9EIC0tPiBNeUluZGV4ZXI6OjpQT0Q7Ck15SW5kZXhlcjo6OlBPRCAtLT4gZ2F0ZXdheWVuZFtnYXRld2F5XTo6OkdBVEVXQVk7CmNsYXNzRGVmIElOU1BFQ1Qgc3Ryb2tlOiNGMjlDOUYKY2xhc3NEZWYgSk9JTl9JTlNQRUNUIHN0cm9rZTojRjI5QzlGCmNsYXNzRGVmIEdBVEVXQVkgZmlsbDpub25lLGNvbG9yOiMwMDAsc3Ryb2tlOm5vbmUKY2xhc3NEZWYgSU5TUEVDVF9BVVhfUEFTUyBzdHJva2UtZGFzaGFycmF5OiAyIDIKY2xhc3NEZWYgSEVBRFRBSUwgZmlsbDojMzJDOENEMUQKCmNsYXNzRGVmIEVYVEVSTkFMIGZpbGw6I2ZmZixzdHJva2U6IzMyQzhDRA==\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "flow.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pCgdQx04h-Zp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e033b9-a566-4910-d6d4-f10d7534e730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used: cuda:0\n",
            "\n",
            "Insert your query ('n' to not pass a query): \n",
            "n\n",
            "Time needed to read 1000 texts: 0.5496423244476318 s\n",
            "Device used: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2312 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time needed to encode 1000 documents: 29.743781805038452\n",
            "Time needed to train and add 1000 texts to the index: 0.39182615280151367 s\n",
            "Time needed to encode 1 documents: 0.016463756561279297\n",
            "Time needed to search 1 query: 5.4836273193359375e-06 s\n",
            "Indexes of the most similar documents: [[  1 509 588 407 535]]\n",
            "with distance: [[4.066163  7.5142684 7.80306   7.922653  8.129744 ]]\n",
            "1th result:\n",
            "Doc Id: 90\n",
            "Title: Good branching and merging tutorials for TortoiseSVN?\n",
            "Text:\n",
            "<p>Are there any really good tutorials explaining <a href=\"http://svnbook.red-bean.com/en/1.8/svn.branchmerge.html\" rel=\"nofollow\">branching and merging</a> with Apache Subversion? </p>\n",
            "\n",
            "<p>All the better if it's specific to TortoiseSVN client.</p>\n",
            "\n",
            "----------------------------\n",
            "1th answer's Id: 1466832, with score: 19 and text:\n",
            "<p>My easy click-by-click instructions (<strong>specific to TortoiseSVN</strong>) are in Stack&nbsp;Overflow question <em><a href=\"http://stackoverflow.com/questions/1461922\">What is the simplest way to do branching and merging using TortoiseSVN?</a></em>.</p>\n",
            "\n",
            "----------------------------\n",
            "2th answer's Id: 92, with score: 13 and text:\n",
            "<p><a href=\"http://svnbook.red-bean.com/\">Version Control with Subversion</a></p>\n",
            "\n",
            "<p>A very good resource for source control in general. Not really TortoiseSVN specific, though.</p>\n",
            "----------------------------\n",
            "3th answer's Id: 202317, with score: 2 and text:\n",
            "<p>You can also try <em><a href=\"http://www.codeproject.com/KB/work/XTortoiseSVN.aspx\" rel=\"nofollow\">Version Control for the Standalone Programmer - Part 1</a></em> or perhaps <em><a href=\"http://www.kenegozi.com/Blog/2007/07/30/merging-with-tortoisesvn.aspx\" rel=\"nofollow\">Merging with TortoiseSVN</a></em>.</p>\n",
            "\n",
            "########################################\n",
            "2th result:\n",
            "Doc Id: 45500\n",
            "Title: Wacom tablet Python interface\n",
            "Text:\n",
            "<p>If possible I want to catch pressure sensitive input from a Wacom tablet in Python. Are there any Python libraries available that can do this?</p>\n",
            "\n",
            "----------------------------\n",
            "1th answer's Id: 45564, with score: 3 and text:\n",
            "<p>You could perhaps take a look at the <a href=\"http://www.alexmac.cc/tablet-apps/tablet-apps-0.3.1.tar.bz2\" rel=\"nofollow\">software</a> described <a href=\"http://www.alexmac.cc/tablet-apps/\" rel=\"nofollow\">here</a>. It is a gnome applet, written in Python.</p>\n",
            "\n",
            "<p>From the web site:</p>\n",
            "\n",
            "<p>\"The gnome wacom applet is a small gnome panel applet that shows how much pressure is being applied to your wacom tablet by the current device. Clicking on the panel icon brings up a dialog allowing you to select a different device and check what pressure and tilt information is being recieved from it. This dialog also contains a small drawing test area to give your pen a quick test.\"</p>\n",
            "\n",
            "<p><a href=\"http://www.google.com/search?q=wacom+tablet+python\" rel=\"nofollow\">Google is your friend</a></p>\n",
            "\n",
            "----------------------------\n",
            "2th answer's Id: 5088138, with score: 2 and text:\n",
            "<p>Use PySide (wrapper for QT)'s QTabletEvent: <a href=\"http://www.pyside.org/docs/pyside/PySide/QtGui/QTabletEvent.html#PySide.QtGui.QTabletEvent\" rel=\"nofollow\">http://www.pyside.org/docs/pyside/PySide/QtGui/QTabletEvent.html#PySide.QtGui.QTabletEvent</a></p>\n",
            "\n",
            "----------------------------\n",
            "3th answer's Id: 10476121, with score: 2 and text:\n",
            "<p>For Mac OS X:</p>\n",
            "\n",
            "<p><a href=\"https://bitbucket.org/AnomalousUnderdog/pythonmactabletlib\" rel=\"nofollow\">https://bitbucket.org/AnomalousUnderdog/pythonmactabletlib</a></p>\n",
            "\n",
            "<blockquote>\n",
            "  <p>A small Python library to allow Python scripts to access pen tablet\n",
            "  input data in Mac OS X.</p>\n",
            "  \n",
            "  <p>The library exists as plain C code compiled as a dynamic\n",
            "  library/shared object. It interfaces with the Mac OS X's API to get\n",
            "  data on pen tablet input.</p>\n",
            "  \n",
            "  <p>Then, Python scripts can use ctypes to get the data.</p>\n",
            "</blockquote>\n",
            "\n",
            "<p>Send me a message if you have any problems with it.</p>\n",
            "\n",
            "----------------------------\n",
            "4th answer's Id: 28655874, with score: 1 and text:\n",
            "<p>Pressure data is available in <a href=\"http://python-gtk-3-tutorial.readthedocs.org/en/latest/\" rel=\"nofollow\">PyGObject to access Gtk+ 3</a> on multiple platforms, though \"Windows users may still want to keep using PyGTK until more convenient installers are published.\" [<a href=\"https://wiki.python.org/moin/PyGtk\" rel=\"nofollow\">citation</a>] Motion event objects generated by pressure sensitive devices will carry pressure data.</p>\n",
            "\n",
            "########################################\n",
            "3th result:\n",
            "Doc Id: 51660\n",
            "Title: Metamodelling tools\n",
            "Text:\n",
            "<p>What tools are available for metamodelling? </p>\n",
            "\n",
            "<p>Especially for developing diagram editors, at the moment trying out <a href=\"http://www.eclipse.org/modeling/gmf/\" rel=\"nofollow\">Eclipse GMF</a></p>\n",
            "\n",
            "<p>Wondering what other options are out there? \n",
            "Any comparison available?</p>\n",
            "\n",
            "----------------------------\n",
            "1th answer's Id: 337043, with score: 3 and text:\n",
            "<p>Your question is simply too broad for a single answer - due to many aspects.</p>\n",
            "\n",
            "<p>First, meta-modelling is not a set term, but rather a very fuzzy thing, including modelling models of models and reaching out to terms like MDA.</p>\n",
            "\n",
            "<p>Second, there are numerous options to developing diagram editors - going the Eclipse way is surely a nice option.</p>\n",
            "\n",
            "<p>To get you at least started in the Eclipse department:</p>\n",
            "\n",
            "<ul>\n",
            "<li>have a look at MOF, that is architecture for \"meta-modelling\" from the OMG (the guys, that maintain UML)</li>\n",
            "<li>from there approach EMOF, a sub set which is supported by the Eclipse Modelling Framework in the incarnation of Ecore.</li>\n",
            "<li>building something on top of GMF might be indeed a good idea, because that's the way existing diagram editors for the Eclipse platform take (e.g. Omondo's EclipseUML)</li>\n",
            "<li>there are a lot of tools existing in the Eclipse environment, that can utilize Ecore - I simply hope, that GMF builts on top of Ecore itself.</li>\n",
            "</ul>\n",
            "\n",
            "----------------------------\n",
            "2th answer's Id: 178585, with score: 1 and text:\n",
            "<p><a href=\"http://www.gnome.org/projects/dia/\" rel=\"nofollow\">Dia</a> has an API for this - I was able to fairly trivially frig their UML editor into a basic ER modelling tool by changing the arrow styles.  With a DB reversengineering tool I found in sourceforge (took the schema and spat out dia files) you could use this to document databases.  While what I did was fairly trivial, the API was quite straightforward and it didn't take me that long to work out how to make the change.</p>\n",
            "\n",
            "<p>If you're of a mind to try out Smalltalk There used to be a Smalltalk meta-case framework called <a href=\"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/nectar-adamb/web/DomeRelease.html\" rel=\"nofollow\">DOME</a> which does this sort of thing.  If you download <a href=\"http://www.cincomsmalltalk.com\" rel=\"nofollow\">VisualWorks</a>, DOME is one of the contributed packages.</p>\n",
            "\n",
            "----------------------------\n",
            "3th answer's Id: 347763, with score: 1 and text:\n",
            "<p>GMF is a nice example. At the core of this sits <a href=\"http://www.eclipse.org/emf/\" rel=\"nofollow\">EMF/Ecore</a>, like computerkram sais. Ecore is also used for the base of Eclipse's <a href=\"http://www.eclipse.org/uml2\" rel=\"nofollow\"> UML2 </a>. The prestige use case and proof of concept for GMF is certainly <a href=\"http://www.eclipse.org/modeling/mdt/?project=uml2tools\" rel=\"nofollow\">UML2 Tools</a>.</p>\n",
            "\n",
            "########################################\n",
            "4th result:\n",
            "Doc Id: 36890\n",
            "Title: Changing a CORBA interface without recompiling\n",
            "Text:\n",
            "<p>I'd like to add a method to my existing server's CORBA interface. Will that require recompiling all clients?</p>\n",
            "\n",
            "<p>I'm using TAO.</p>\n",
            "\n",
            "----------------------------\n",
            "1th answer's Id: 42467, with score: 4 and text:\n",
            "<p>Recompilation of clients is not required (and should not be, regardless of the ORB that you use). As Adam indicated, lookups are done by operation name (a straight text comparison).</p>\n",
            "\n",
            "<p>I've done what you're describing with our ACE/TAO-based system, and encountered no issues (servers were in ACE/TAO C++, clients were ACE/TAO C++, C# using Borland's Janeva, and OmniORBPy).</p>\n",
            "\n",
            "----------------------------\n",
            "2th answer's Id: 42484, with score: 3 and text:\n",
            "<p>Assuming that the clients and servers are communicating via IIOP, no recompilation is required.  An IIOP message contains the name of the interface, the name of the method, and the parameters.  If none of those things have changed, then everything should remain compatible.  Adding another method to the interface won't change any of those existing things.</p>\n",
            "\n",
            "<p>On the other hand, if your objects are using a different protocol, or if the clients are in-process with the server and thus bypassing IIOP, you may need to make sure everything gets recompiled.</p>\n",
            "\n",
            "----------------------------\n",
            "3th answer's Id: 36895, with score: 1 and text:\n",
            "<p>Operations (methods) are looked-up by name, so you only need to recompile the clients that use the new operation.</p>\n",
            "\n",
            "----------------------------\n",
            "4th answer's Id: 96505, with score: 1 and text:\n",
            "<p>Clients using colocation (i.e. running within the same process with colocation enabled in ORB) must be recompiled. Remote clients may remain the same - as said previously, methods are matched by symbolic name.</p>\n",
            "\n",
            "########################################\n",
            "5th result:\n",
            "Doc Id: 48250\n",
            "Title: Free JSP plugin for eclipse?\n",
            "Text:\n",
            "<p>I was looking out for a free plugin for developing/debugging JSP pages in eclipse.<br />\n",
            "Any suggestions? </p>\n",
            "\n",
            "----------------------------\n",
            "1th answer's Id: 48394, with score: 4 and text:\n",
            "<p>The <a href=\"http://wiki.eclipse.org/Category:Eclipse_Web_Tools_Platform_Project\" rel=\"nofollow\">Eclipse Web Tools Platform Project</a> includes a JSP debugger. I have only ever needed to use it with Tomcat so I cannot say how well it works with other servlet containers.</p>\n",
            "\n",
            "----------------------------\n",
            "2th answer's Id: 48399, with score: 3 and text:\n",
            "<p>BEA seems to have a free one <a href=\"http://commerce.bea.com/products/workshop/workshop_prod_fam.jsp\" rel=\"nofollow\">BEA JSP plugin</a> - not used it, so not sure how good it is.</p>\n",
            "\n",
            "<p>Oracle now owns BEA, and they have this <a href=\"http://www.oracle.com/technetwork/developer-tools/workshop/overview/index.html\" rel=\"nofollow\">plugin</a> which might do a similar job.</p>\n",
            "\n",
            "----------------------------\n",
            "3th answer's Id: 51455, with score: 3 and text:\n",
            "<p>The former BEA Workshop is now <a href=\"http://www.oracle.com/technology/software/products/ias/bea_main.html#devtools\" rel=\"nofollow\">Oracle Workshop</a>. It is the best JSP editor with WYSIWYG support and it is free. It is not specific to WebLogic. Basic JSP editing is server neutral anyway. However, it supports launching and debugging on many servers.</p>\n",
            "\n",
            "<p>You can read my <a href=\"http://blog.zvikico.com/2008/08/the-best-jspstrutsjsf-development-tool-is-now-free.html\" rel=\"nofollow\">blog post</a> about it.</p>\n",
            "\n",
            "########################################\n"
          ]
        }
      ],
      "source": [
        "query_encoder = MyTransformer(device=device)\n",
        "num_docs = 1000 #number of documents to read, 0 to load the whole dataset\n",
        "\n",
        "if(device.type!=\"cpu\"): #if you are using GPU\n",
        "  print(\"\\nInsert your query ('n' to not pass a query): \")\n",
        "  q = input()\n",
        "  if(q=='n'): #use a question from the dataset as query\n",
        "    test_with_gpu(num_docs, index_output = index_outfile, index_file = index_outfile)\n",
        "  else:\n",
        "    test_with_gpu(num_docs, q, index_output = index_outfile, index_file = index_outfile)\n",
        "else: #if you are not using GPU\n",
        "  with flow:\n",
        "      docArray = DocumentArray([Document(id=questions.iloc[i].Id, text=questions.iloc[i].Body) for i in range(num_docs)])\n",
        "      flow.index(docArray, index_output = index_outfile, index_file = index_outfile)\n",
        "      while(True):\n",
        "        print(\"\\nInsert your query (q to quit): \")\n",
        "        q = input()\n",
        "        if (q == \"q\"):\n",
        "          print(\"\\n\\nGoodbye!\")\n",
        "          break\n",
        "        query_encoder.encode(DocumentArray([q]))\n",
        "        d = flow.search(\n",
        "            inputs=DocumentArray(q)\n",
        "            )\n",
        "        get_answers(d, questions, answers)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0RbtMSoV-U_0"
      ],
      "name": "Romani_DM_project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}